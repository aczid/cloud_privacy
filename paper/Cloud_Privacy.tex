\documentclass[11pt]{article}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{tabularx}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{sidecap}
\usepackage{epstopdf}

%\def\bibfont{\scriptsize}

\usepackage{natbib}
\setlength{\bibsep}{0.0pt}

\begin{document}
\title{Privacy in Cloud Computing}

\author{Erik Boss \and Aram Verstegen}
%\institute{Radboud Universiteit Nijmegen}
\date{\today}
\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}
In recent years there has been a lot of positive buzz surrounding the idea of \textit{cloud computing}: a systems design trend that allows applications to run as ubiquitous services by providing abstractions to the lower layers of application architecture.
These layers can be distinguished as \textit{Software}, \textit{Platform} and \textit{Infrastructure} and each of these has a different target audience which can buy these facilities ``as a service'', i.e., Software/Platform/Infrastructure as a Service (SaaS, PaaS, IaaS). 
Such abstractions bring benefits like implicit redundancy, centralized security measures and, most importantly, reduces capital and operational expenses for customers by the increased efficiency at which each layer can be managed.

That being said, there is also a note of concern with things like centralized information management, giving up physical segregation for logical segregation, near-infinite storage capacity, cheap, ubiquitous computing power and the legal issues surrounding information processing systems whose infrastructure scale transcends national borders.

In cloud computing it's apparent that there is a very strong connection between security and privacy.
If any central component of cloud infrastructure were to be compromised it could almost certainly be used to disclose very privacy-sensitive information, given the scale of cloud infrastructure providers and what kind of private information is (or can be) recorded using applications running ``in the cloud''. In fact, there is no proof or technically feasible way to prove that such a compromise hasn't occurred.

In this paper we will go over some of the potential and actual privacy-related problems we have encountered as part of our study on the technical and legal domains of cloud computing. Starting off, in \autoref{sec:problems_in_cloud_computing} we shall give a fairly broad overview of the issues involved with cloud computing.
This will lead into an analysis of the current legal situation regarding cloud computing in \autoref{sec:legal_analysis}. In \autoref{sec:pet} and \autoref{sec:technical_analysis} we will go over the technical situation, specifically describing some of the solutions to privacy-related problems in cloud computing by means of Privacy Enhancing Techologies (PETs).

\section{Privacy problems in cloud computing}
\label{sec:problems_in_cloud_computing}
%General concerns with cloud security: centralisation, single point of failure, corporate espionage, subpoenas to the provider, impact of security compromises
%In the multi-layered design of cloud-based infrastructure our privacy concerns are mostly contained within individual layers.

While centralization of infrastructure should generally imply better basic physical and network security, it may lead to greater fragility of the networked ecosystem as a whole, whereby attacks have greater impact than in a strictly distributed environment.
Consider how acts of observation, manipulation or destruction would be more effective when carried out against the central node in a star-shaped network rather than anywhere in a well-connected network.
There exists more redundancy in the more connected network, which helps make the infrastructure more robust against any such disruptions.
%For a trustworthy internet we want to have more connection points into the network than the adversary can conceivably take over.
%Some systems may appear to be more (logically) distributed than they actually (physically) are. 

The main concern in related research is surrounding data privacy, but we find computational privacy to be of equal or greater concern.
Remotely hosted applications come with the inherent problem that their behaviour can be more difficult to analyse in order asses their security/privacy.
Whereas with personal computing the integrity of the system could be verified all the way down to the hardware level, the cloud environment requires trust in providers' ability to adequately handle privacy-sensitive information.
The problem is exacerbated by the fact that virtualization technologies abstract systems from the hardware layer, and require systems built upon them to similarly trust this technology during computation.
And again there is centralization in these technologies, leading to concerns about (virtually) universal backdoors in all systems built upon a handful of such technologies.
\cite{chow2009controlling}

%We want to cover either of these two cases:

% Dit heeft eigenlijk niks met cloud privacy te maken... NSLs mogen alleen transactionele data opvragen
%\subsection{Problem case summary: Doe vs Ashcroft}
%In the United States there exists an investigative tool
%An investigative tool for intelligence agencies like the FBI known to be in use since 1978 is the National Security Letter (NSL): a demand letter for transactional records, accompanied by an indefinite gag order forbidding disclosure of the demand.
%Summary of the story of an ISP that got a NSL - and what an NSL actually demands
%\cite{garlinger2009privacy, gorham2008national}

%\subsection{Problem case summary: Cyberlocker raids}
%How cloud providers are being held accountable for the content they host

%\subsection{The dawn of personal cloud storage privacy}
% TODO how many providers to list?
Cloud providers like MediaFire, 4shared and the former MegaUpload provide a personal storage platform through the web for file sharing and archiving, be it public or private.
Such services, known as `cyberlockers' have become popular over the past decade or so, to the point where they can be considered ubiquitous.
The larger of these websites have in fact become so popular that they are in the lead for most visited sites and are among the most significant global internet bandwidth users.
Sharing services like these are known to be used as trafficking vehicles for pirated copyrighted media as they can provide free storage in a way that in most cases can be used anonymously, and can even generate profit for the uploader.
One of such companies was the former MegaUpload: a company which had its assets seized by the United States Department of Justice in early 2012 on claims of illegal file sharing activity, in what is still unfolding as a particularly complex legal case.
The company has since reinvented itself as MEGA, providing the same services but boasting better privacy and security by using cryptographic methods to mask the stored data.
Following the legal prosecution, other companies like Ciphercloud and SpiderOak have also brought forth solutions to similar concerns.
While MEGA and SpiderOak seem to have taken a cue from the cryptography community and chose an open design to be verifiable by its users, Ciphercloud have recently become the center of controversy after they tried to file a DMCA takedown notice against a StackExchange thread in which their cryptography was analyzed and consequently decimated.
% TODO source ciphercloud
%We have decided to analyse the security of MEGA's design in \autoref{sec:cyberlocker}.


\section{Legal analysis}
\label{sec:legal_analysis}

In this section we will review the relevant legal aspects surrounding cloud computing. We shall focus on the European legal framework since it is the most extensive.

The key legislature is the EU Data Protection Directive 95/46/EC, which provides basic rights to consumers' data by obligating data collectors to uphold their right to privacy. \cite{directive199595}
This directive is based on seven key principles surrounding citizen privacy.
They are as follows:
\begin{itemize}
\item Data subjects should be given notice of collection;
\item Data should only be used for the purposes specified;
\item Data should not be disclosed without the subject's consent;
\item The data processors should uphold adequate security;
\item It should be disclosed to data subjects who is processing their information;
\item Data subjects should be allowed to access and update their data;
\item It should be possible to hold data collectors accountable to these principles.
\end{itemize}

This legislature is set to be succeeded by the General Data Protection Regulation, which amends the code to extend its reach beyond the EU, and applies everywhere information about EU citizens is collected.
It also mandates that data breaches must be reported to the Data Protection Authority, and stipulates serious fines for failure to comply.
Note, however, that this regulation is still under review and thus subject to change.

Complementary to the Data Protection Directive is the E-privacy Directive (Directive on Privacy and Electronic Communications), which also protects citizens from excessive data retention, spam e-mail and the use of tracking cookies without explicit consent.
This document spawned the much-debated ``Cookie law'' (Cookiewet) in the Netherlands.

In a cloud computing setting, the main legal problems boil down to two issues: accountability of non-EU companies and multi-tenancy of cloud applications. Essentially, the problems emerge from different users belonging to different jurisdicitions using a system that is possibly in yet another jurisdiction. Try enforcing anything when the user lives in Europe, accessing an Asian server of an American company. Problems aplenty, we should think.

The type of cloud system, i.e., ``the X in XaaS'', also has its part to play in the legal setting of cloud computing.
From IaaS (infrastructure) to SaaS (software), the responsibilities concerning privacy of provider and client change.
In an IaaS setting, the provider can only do so much to provide privacy safeguards.
The responsibilities lie primarily in providing availability of the infrastructure as well as providing hypervisor and physical security.
The client, however is responsible for building secure applications or platforms on top of this infrastructure.
In a SaaS setting, the provider needs to provide more guarantees.
A user needs to be sure that the software is adequately secured on infrastructure, platform and software level.
Obviously, in the PaaS setting the responsibilities lie somewhere halfway between IaaS and SaaS. The provider needs to provide a secure platform but it is the client's job to provide the necessary safeguards for any application built on top of this platform.

We like to note that, when applying the sort of the solutions we describe in \autoref{sec:pet} we process significantly less personal data. Say, for instance, that we have a situation in which we want to search in a database. Now, if we apply techniques to search using encrypted queries in an encrypted database, we stop processing any personal data that is not linked to the communications protocol (like IP addresses). Such techniques, are in a way a win-win solution. They reduce liability risks for providers and provide privacy for users.

As a side note, the United States Department of Commerce has worked with the EU to develop the Data Protection Directive principles into what they call ``Safe Harbor'' principles which US companies can opt-in to.
This essentially allowed US companies to do business with the EU, which has the more stringent data protection legislation.
Also, the Digital Millennium Copyright Act (DMCA) recognizes the concept of a ``mere conduit'' (of information) as did the European Data Protection Directive. \cite{congress1998digital}
It should be noted that, currently, the protections the EU legislation provides do not extend beyond its borders.
Even after the new legislation comes into effect, the PATRIOT act~\cite{mailman2002uniting} still allows the United States to demand access to any businesses that operate there in the interest of national security.

% In hindsight, doing contract law was probably a bad idea, except for the auditing standards mentioned above..
% \subsection{Contract law}
% SLAs and how undesirable some of their provisions are


% Article 29 WP opinion (draft, nog geen reference)

%\subsection{Case law}
%Lindqvist case

% Legal responsibilities as a function of the type of cloud architecture (see slides for the idea)

\section{Privacy-Enhancing Technologies}
\label{sec:pet}

Certain recent technologies provide ways to interface with cloud services with some measure of privacy.
Two of these privacy-enhancing technologies are \emph{search under encryption} and \emph{reusable garbled secrets through function encryption}.
Both come from the realm of cryptography and it should be noted that it is likely that encryption is never enough to provide privacy in the general case of cloud applications \cite{van2010impossibility}.

\subsection{Search under Encryption}

\subsection{Reusable Garbled Circuits}

Goldwasser \cite{goldwasser2013reusable} recently provided the means to reuse so-called garbled circuits by means of a functional encryption scheme using \emph{succinct} functions.
This allows the computation of succinct functions on encrypted data without learning anything but the result of the function.
To reuse Goldwasser's example, this allows us to, say, compute a spam-detection function on encrypted mail that learns nothing about the contents of said mail except whether it was classified as spam.
We shall not go into the details of the security properties of this system but we shall attempt to indicate how they work.

Essentially, a garbled circuit allows computation of a given function $f(x)$ without leaking information about $f$ or $x$.
The only information learned is the output, $f(x)$.
More precisely, we have a circuit $C$ that computes the respective function given input $x$.
We garble the circuit by encoding it using some one-way function, this outputs the garbled circuit $C'$ and a secret key $sk$.
The input $x$ is also encoded in order to fit the newly generated circuit.
These circuits basically need to provide two things: the privacy of $x$ and the privacy of $C$ (input privacy and circuit privacy, respectively).
Historically, such a garbled circuit was for one-time use only, new input required new encodings.
What Goldwasser did is to provide a way to, with only one key and thus one encoding, apply the circuit to a polynomial number of inputs.

As stated, in order to reuse such circuits we need a function encryption (FE) scheme.
In such a scheme, we have a master public key $mpk$ that anyone can encrypt data with.
Furthermore, the data owner can issue keys for functions that given a key $sk_f$ and ciphertext $c$ corresponding to input $x$ can compute $f(x)$ on $c$ using $sk_f$.
In this case we rely on homomorphic encryption to make sure that we can actually compute on encrypted data in a semantically secure way.

Now that the components are laid out, we shall go on to describe, very globally, the workings of Goldwasser's reusable garbled circuits.
As mentioned, we use fully homomorphic encryption (FHE) in order to compute on encrypted data.
The security of FHE guarantees no information leaks on the input $x$, therefore providing the input privacy that we need.
We now use a functional encryption scheme using FHE to compute functions $f$ on given input.
Still lacking is the ability to provide circuit privacy.
In Goldwasser's scheme, this is accomplished by garbling circuit $C$ using a semantically secure encryption scheme in combination with said functional encryption scheme.
This yields the garbled circuit $U$ and a secret key for $U$ such that given $sk$ and $x$, U will decrypt $C$ and yields $C(x)$.
In this scheme, we can reuse the circuit for a polynomial amount of inputs.

For a precise definition of the functional encryption scheme used and the reusable garbled circuit that follows we refer the reader to Goldwasser's excellent paper; this section was only intended as a global overview.

In a cloud setting we can think of several services being given keys for functions corresponding to the service they provide.
All these services could operate on any data encrypted with $mpk$, regardless of origin.
One such service might provide search functionality, as described in the previous section.
Another might provide some consistency check of the underlying plaintext data.

% Only store encrypted data, Let clients perform crypto so it can be validated externally
% \cite{kamara2010cryptographic}

% Homomorphic encryption,
% Searching in encrypted data \cite{harrower2009searching, li2010fuzzy}


\section{Technical analysis}
\label{sec:technical_analysis}
%Security considerations in cloud infrastructure

%Summary of techniques for security, focussing on Confidentiality and Integrity as we know them from general computing

%\subsection{Hypervisor security}
%``Reflections on Trusting Trust'' by Kevin Thompson \cite{thompson1984reflections}

Hardware backdoors \cite{sparks2009chipset, duflot2010limits}

Reverse engineering requirement
\cite{rutkowska2008bluepilling}

\subsection{Case study: private cyberlocker}
\label{sec:cyberlocker}
%How MEGA/Megaupload tried to cover themselves (and to a lesser extent their users) from the burden of accountability for the data they host
%
%Our ideas to keep private data private in cloud storage without sacrificing de-duplication
%(subject to further experimentation)
In the market of so-called `cyberlocker' sites most don't use (or boast) any privacy enhancing technologies.
MEGA and SpiderOak do, and both make use of a layered approach whereby files are encrypted with strong random passwords, which are then encrypted by a single master password associated to the user.
Assuming the underlying cipher and random generation algorithms are implemented in a cryptographically secure way such a scheme is seems quite secure: the files nor their keys never reach the server in the plain.
Their approaches diverge when it comes to application delivery: where MEGA opted for in-browser delivery of content over the HTTPS channel SpiderOak chose to draft a native application its service to better ensure application integrity (as it runs in a separate context).
The SpiderOak client so far remains a closed-source application, but at least its code and behaviour can be audited and validated using traditional means in contrast to MEGAs due to their choice of delivery environment.


%\subsection{Past attacks}
%Past security vulnerabilities (that threaten privacy)
%\cite{meer2009clobbering}

\section{Conclusion}
So far we find the legislature adequate, but the technical solutions to hold cloud providers accountable severely limited.

%\section{References}

\bibliography{Cloud_Privacy}{}
\bibliographystyle{plain}

\end{document}

% vim:tw=0:wrap

